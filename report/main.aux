\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@cite{neyshabur2018towards}
\abx@aux@segm{0}{0}{neyshabur2018towards}
\abx@aux@cite{zagoruyko2016wide}
\abx@aux@segm{0}{0}{zagoruyko2016wide}
\abx@aux@cite{zhang2016understanding}
\abx@aux@segm{0}{0}{zhang2016understanding}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\abx@aux@page{1}{2}
\abx@aux@page{2}{2}
\abx@aux@page{3}{2}
\abx@aux@cite{rolnick2017deep}
\abx@aux@segm{0}{0}{rolnick2017deep}
\abx@aux@segm{0}{0}{neyshabur2018towards}
\abx@aux@segm{0}{0}{neyshabur2018towards}
\abx@aux@cite{goldt2019dynamics}
\abx@aux@segm{0}{0}{goldt2019dynamics}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Formal background}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}}
\newlabel{section:prior}{{2}{3}{Related work}{section.2}{}}
\abx@aux@page{4}{3}
\abx@aux@page{5}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Self-regularization of over-parameterized models}{3}{subsection.2.1}}
\abx@aux@page{6}{3}
\abx@aux@page{7}{3}
\abx@aux@cite{casper2019removable}
\abx@aux@segm{0}{0}{casper2019removable}
\abx@aux@cite{martin2018implicit}
\abx@aux@segm{0}{0}{martin2018implicit}
\abx@aux@cite{nagarajan2019uniform}
\abx@aux@segm{0}{0}{nagarajan2019uniform}
\abx@aux@cite{tian2019luck}
\abx@aux@segm{0}{0}{tian2019luck}
\abx@aux@cite{frankle2018lottery}
\abx@aux@segm{0}{0}{frankle2018lottery}
\abx@aux@cite{brutzkus2019larger}
\abx@aux@segm{0}{0}{brutzkus2019larger}
\abx@aux@page{8}{4}
\abx@aux@page{9}{4}
\abx@aux@page{10}{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Advantageous properties of over-parameterized models}{4}{subsection.2.2}}
\abx@aux@page{11}{4}
\abx@aux@page{12}{4}
\abx@aux@cite{han2015learning}
\abx@aux@segm{0}{0}{han2015learning}
\abx@aux@cite{li2016pruning}
\abx@aux@segm{0}{0}{li2016pruning}
\abx@aux@segm{0}{0}{frankle2018lottery}
\abx@aux@cite{liu2018rethinking}
\abx@aux@segm{0}{0}{liu2018rethinking}
\abx@aux@cite{frankle2019lottery}
\abx@aux@segm{0}{0}{frankle2019lottery}
\abx@aux@cite{morcos2019one}
\abx@aux@segm{0}{0}{morcos2019one}
\abx@aux@page{13}{5}
\abx@aux@page{14}{5}
\abx@aux@page{15}{5}
\abx@aux@page{16}{5}
\abx@aux@page{17}{5}
\abx@aux@segm{0}{0}{brutzkus2019larger}
\abx@aux@page{18}{6}
\abx@aux@page{19}{6}
\abx@aux@page{20}{6}
\abx@aux@cite{lecun1990optimal}
\abx@aux@segm{0}{0}{lecun1990optimal}
\abx@aux@cite{hassibi1993second}
\abx@aux@segm{0}{0}{hassibi1993second}
\abx@aux@segm{0}{0}{li2016pruning}
\abx@aux@cite{suau2019filter}
\abx@aux@segm{0}{0}{suau2019filter}
\abx@aux@cite{bartoldson2019generalization}
\abx@aux@segm{0}{0}{bartoldson2019generalization}
\abx@aux@cite{namhoon2018SNIP}
\abx@aux@segm{0}{0}{namhoon2018SNIP}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Leveraging model simplicity: Pruning and Compression}{7}{subsection.2.3}}
\abx@aux@page{21}{7}
\abx@aux@page{22}{7}
\abx@aux@page{23}{7}
\abx@aux@page{24}{7}
\abx@aux@page{25}{7}
\abx@aux@segm{0}{0}{liu2018rethinking}
\abx@aux@cite{lecun1998gradient}
\abx@aux@segm{0}{0}{lecun1998gradient}
\abx@aux@cite{simonyan2014very}
\abx@aux@segm{0}{0}{simonyan2014very}
\abx@aux@cite{glorot2010understanding}
\abx@aux@segm{0}{0}{glorot2010understanding}
\abx@aux@segm{0}{0}{lecun1998gradient}
\abx@aux@segm{0}{0}{simonyan2014very}
\abx@aux@segm{0}{0}{glorot2010understanding}
\abx@aux@page{26}{8}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}{section.3}}
\abx@aux@page{27}{8}
\abx@aux@segm{0}{0}{nagarajan2019uniform}
\abx@aux@segm{0}{0}{brutzkus2019larger}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architectures tested in this project. Convolutions are 3x3. Lenet is from \cite {lecun1998gradient}. Conv-2/4/6 are variants of VGG \nobreakspace  {}\autocite {simonyan2014very}. Initializations are Gaussian Glorot \nobreakspace  {}\autocite {glorot2010understanding}.\relax }}{9}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparison of pruning criteria}{9}{subsection.3.1}}
\abx@aux@page{34}{9}
\abx@aux@page{35}{9}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lenet-fc1}{{2a}{11}{Pruning the first fully connected layer of LeNet\relax }{figure.caption.11}{}}
\newlabel{sub@fig:lenet-fc1}{{a}{11}{Pruning the first fully connected layer of LeNet\relax }{figure.caption.11}{}}
\newlabel{fig:conv2-fc1}{{2b}{11}{Pruning the first fully connected layer of Conv-2\relax }{figure.caption.11}{}}
\newlabel{sub@fig:conv2-fc1}{{b}{11}{Pruning the first fully connected layer of Conv-2\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of different pruning metrics. \texttt  {max\_mag\_pruning} (\texttt  {min\_mag\_pruning}) corresponds to \textit  {Magnitude pruning} that maximizes (minimizes) the Frobenius norm of the pruned matrix. \texttt  {max\_fp\_pruning} (\texttt  {min\_fp\_pruning}) corresponds to \textit  {Frame-potential pruning}, it maximizes (minimizes) the frame-potential of the pruned matrix. For random pruning, averages were taken over 5 random experiments. Error bars were added with a width equal to the standard deviation, computed over the 5 random experiments, of the test accuracy obtained after pruning $i$ units.\relax }}{11}{figure.caption.11}}
\abx@aux@segm{0}{0}{martin2018implicit}
\abx@aux@segm{0}{0}{nagarajan2019uniform}
\abx@aux@segm{0}{0}{brutzkus2019larger}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Visualizing training dynamics}{12}{subsection.3.2}}
\abx@aux@page{36}{12}
\abx@aux@page{37}{12}
\abx@aux@page{38}{13}
\newlabel{fig:conv2-fc1-init}{{3a}{14}{At initialization\relax }{figure.caption.14}{}}
\newlabel{sub@fig:conv2-fc1-init}{{a}{14}{At initialization\relax }{figure.caption.14}{}}
\newlabel{fig:conv2-fc1-final}{{3b}{14}{After convergence\relax }{figure.caption.14}{}}
\newlabel{sub@fig:conv2-fc1-final}{{b}{14}{After convergence\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualizing the training dynamics of the first fully connected layer of Conv-2\relax }}{14}{figure.caption.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Cluster formation in the last convolutional layer of VGG19 during training\relax }}{15}{figure.caption.15}}
\newlabel{fig:vgg-conv16-final}{{4}{15}{Cluster formation in the last convolutional layer of VGG19 during training\relax }{figure.caption.15}{}}
\abx@aux@cite{lebedev2014speeding}
\abx@aux@segm{0}{0}{lebedev2014speeding}
\abx@aux@segm{0}{0}{martin2018implicit}
\newlabel{fig:conv2-sgd-resampled}{{5a}{17}{Last convolutional layer, inner-product cutoff: 0.48. All nodes are only connected to their image in the other model, which was identical to it at initialization.\relax }{figure.caption.17}{}}
\newlabel{sub@fig:conv2-sgd-resampled}{{a}{17}{Last convolutional layer, inner-product cutoff: 0.48. All nodes are only connected to their image in the other model, which was identical to it at initialization.\relax }{figure.caption.17}{}}
\newlabel{fig:fc1-sgd-resampled}{{5b}{17}{First fully connected layer, inner-product cutoff: 0.25\relax }{figure.caption.17}{}}
\newlabel{sub@fig:fc1-sgd-resampled}{{b}{17}{First fully connected layer, inner-product cutoff: 0.25\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Model comparison after training, when re-sampling the classifier. Red nodes correspond to weight vectors of the initial model, blue nodes to weight vectors of the model with re-sampled classifier.\relax }}{17}{figure.caption.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Subspace analysis}{18}{subsection.3.3}}
\abx@aux@page{39}{18}
\abx@aux@page{40}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Empirical spectral densities of Conv-2 first fully connected layer correlation matrix, $Q = 64$.\relax }}{19}{figure.caption.21}}
\newlabel{fig:conv2-fc1-svd-comp}{{6}{19}{Empirical spectral densities of Conv-2 first fully connected layer correlation matrix, $Q = 64$.\relax }{figure.caption.21}{}}
\newlabel{fig:vgg-conv16-svd-comp}{{7a}{20}{First fully connected layer, $Q = 1$.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:vgg-conv16-svd-comp}{{a}{20}{First fully connected layer, $Q = 1$.\relax }{figure.caption.22}{}}
\newlabel{fig:vgg-conv16-svd-comp}{{7b}{20}{Last convolutional layer, $Q = 9$.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:vgg-conv16-svd-comp}{{b}{20}{Last convolutional layer, $Q = 9$.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Empirical spectral densities of VGG19 layer correlation matrices\relax }}{20}{figure.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Low-rank approximation of first fully connected layer, tested with different architectures. For Conv-2 and Conv-6, values are averages taken over 10 random experiments. $a_{\text  {proj}}$ denotes the accuracy obtained after replacing $\mathbf  {W}$ by $\mathbf  {W}_k$.\relax }}{20}{figure.caption.23}}
\newlabel{fig:low-rank}{{8}{20}{Low-rank approximation of first fully connected layer, tested with different architectures. For Conv-2 and Conv-6, values are averages taken over 10 random experiments. $a_{\text {proj}}$ denotes the accuracy obtained after replacing $\mathbf {W}$ by $\mathbf {W}_k$.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{21}{section.4}}
\abx@aux@page{41}{21}
\abx@aux@page{42}{21}
\abx@aux@page{43}{21}
\abx@aux@page{44}{21}
\abx@aux@page{45}{21}
\abx@aux@page{46}{22}
\abx@aux@page{47}{22}
\abx@aux@page{48}{22}
\abx@aux@page{49}{22}
\abx@aux@page{50}{22}
\abx@aux@page{51}{22}
\abx@aux@page{52}{22}
\abx@aux@page{53}{22}
\abx@aux@page{54}{22}
\abx@aux@page{55}{22}
\abx@aux@page{56}{22}
\abx@aux@page{57}{22}
\abx@aux@page{58}{22}
\abx@aux@page{59}{22}
\abx@aux@page{60}{22}
\abx@aux@page{61}{22}
\abx@aux@page{62}{22}
\abx@aux@page{63}{22}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{bartoldson2019generalization}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{brutzkus2019larger}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{casper2019removable}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{frankle2018lottery}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{frankle2019lottery}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{glorot2010understanding}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{goldt2019dynamics}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{han2015learning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{hassibi1993second}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lebedev2014speeding}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lecun1998gradient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lecun1990optimal}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{namhoon2018SNIP}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{li2016pruning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{liu2018rethinking}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{martin2018implicit}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{morcos2019one}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{nagarajan2019uniform}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{neyshabur2018towards}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rolnick2017deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014very}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tian2019luck}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{suau2019filter}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{zagoruyko2016wide}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{zhang2016understanding}{nyt/global//global/global}
\abx@aux@page{64}{23}
\abx@aux@page{65}{23}
