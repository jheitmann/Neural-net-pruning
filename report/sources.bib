@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}


@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@article{frankle2019lottery,
  title={The Lottery Ticket Hypothesis at Scale},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@article{morcos2019one,
  title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  author={Morcos, Ari S and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  journal={arXiv preprint arXiv:1906.02773},
  year={2019}
}

@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv preprint arXiv:1705.10694},
  year={2017}
}

@article{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1906.08632},
  year={2019}
}

@article{casper2019removable,
  title={Removable and/or Repeated Units Emerge in Overparametrized Deep Neural Networks},
  author={Casper, Stephen and Boix, Xavier and D'Amario, Vanessa and Guo, Ling and Vinken, Kasper and Kreiman, Gabriel},
  journal={arXiv preprint arXiv:1912.04783},
  year={2019}
}

@article{martin2018implicit,
  title={Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning},
  author={Martin, Charles H and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1810.01075},
  year={2018}
}

@article{tian2019luck,
  title={Luck Matters: Understanding Training Dynamics of Deep ReLU Networks},
  author={Tian, Yuandong and Jiang, Tina and Gong, Qucheng and Morcos, Ari},
  journal={arXiv preprint arXiv:1905.13405},
  year={2019}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{lebedev2014speeding,
  title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
  author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1412.6553},
  year={2014}
}


@inproceedings{brutzkus2019larger,
  title={Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={International Conference on Machine Learning},
  pages={822--830},
  year={2019}
}

@inproceedings{brutzkus2019larger,
  title={Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={International Conference on Machine Learning},
  pages={822--830},
  year={2019}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}

@inproceedings{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11611--11622},
  year={2019}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}


